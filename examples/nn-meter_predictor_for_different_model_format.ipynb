{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Use nn-Meter for different model format\n",
    "In this notebook, we showed nn-Meter examples of latency prediction for different model formats of Tensorflow, PyTorch, ONNX."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# list all supporting latency predictors\n",
    "import nn_meter\n",
    "predictors = nn_meter.list_latency_predictors()\n",
    "for p in predictors:\n",
    "    print(f\"[Predictor] {p['name']}: version={p['version']}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Predictor] cortexA76cpu_tflite21: version=1.0\n",
      "[Predictor] adreno640gpu_tflite21: version=1.0\n",
      "[Predictor] adreno630gpu_tflite21: version=1.0\n",
      "[Predictor] myriadvpu_openvino2019r2: version=1.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# define basic information\n",
    "__test_models_folder__ = '../data'\n",
    "os.makedirs(__test_models_folder__, exist_ok=True)\n",
    "\n",
    "# specify basic predictor\n",
    "predictor_name = 'adreno640gpu_tflite21' # user can change text here to test other predictors\n",
    "predictor_version = 1.0\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Use nn-Meter for Tensorflow pb File"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import nn_meter\n",
    "\n",
    "# download data and unzip\n",
    "ppath = os.path.join(__test_models_folder__, \"pb_models\")\n",
    "if not os.path.isdir(ppath):\n",
    "    os.mkdir(ppath)\n",
    "    url = \"https://github.com/microsoft/nn-Meter/releases/download/v1.0-data/pb_models.zip\"\n",
    "    nn_meter.download_from_url(url, ppath)\n",
    "\n",
    "test_model_list = glob(ppath + \"/**.pb\")\n",
    "\n",
    "# load predictor\n",
    "predictor = nn_meter.load_latency_predictor(predictor_name, predictor_version)\n",
    "\n",
    "# predict latency\n",
    "result = {}\n",
    "for test_model in test_model_list:\n",
    "    latency = predictor.predict(test_model, model_type=\"pb\") # in unit of ms\n",
    "    result[os.path.basename(test_model)] = latency\n",
    "    print(f'[RESULT] predict latency for {test_model}: {latency} ms')\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[RESULT] predict latency for ../data/pb_models/alexnet_0.pb: 13.124763483485053 ms\n",
      "[RESULT] predict latency for ../data/pb_models/densenet_0.pb: 73.65728637938379 ms\n",
      "[RESULT] predict latency for ../data/pb_models/googlenet_0.pb: 34.50815902636508 ms\n",
      "[RESULT] predict latency for ../data/pb_models/mnasnet_0.pb: 13.72939336097471 ms\n",
      "[RESULT] predict latency for ../data/pb_models/mobilenetv1_0.pb: 13.972147254154745 ms\n",
      "[RESULT] predict latency for ../data/pb_models/mobilenetv2_0.pb: 10.15371207191722 ms\n",
      "[RESULT] predict latency for ../data/pb_models/mobilenetv3large_0.pb: 9.989918007478076 ms\n",
      "[RESULT] predict latency for ../data/pb_models/mobilenetv3small_0.pb: 4.489849402954042 ms\n",
      "[RESULT] predict latency for ../data/pb_models/proxylessnas_0.pb: 12.509469696629518 ms\n",
      "[RESULT] predict latency for ../data/pb_models/resnet18_0.pb: 39.32351677226427 ms\n",
      "[RESULT] predict latency for ../data/pb_models/resnet34_0.pb: 74.8891391278198 ms\n",
      "[RESULT] predict latency for ../data/pb_models/resnet50_0.pb: 91.73126828870865 ms\n",
      "[RESULT] predict latency for ../data/pb_models/shufflenetv2_0.pb: 5.423898780782251 ms\n",
      "[RESULT] predict latency for ../data/pb_models/squeezenet_0.pb: 18.074222853615616 ms\n",
      "[RESULT] predict latency for ../data/pb_models/vgg11_0.pb: 109.77864175998361 ms\n",
      "[RESULT] predict latency for ../data/pb_models/vgg13_0.pb: 158.90960820442803 ms\n",
      "[RESULT] predict latency for ../data/pb_models/vgg16_0.pb: 219.24169918220582 ms\n",
      "[RESULT] predict latency for ../data/pb_models/vgg19_0.pb: 279.57379015998373 ms\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Use nn-Meter for PyTorch model\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "import os\n",
    "import torchvision.models as models\n",
    "import nn_meter\n",
    "\n",
    "torchvision_models = {\n",
    "    \"resnet18\": models.resnet18(),\n",
    "    \"alexnet\": models.alexnet(),\n",
    "    \"vgg16\": models.vgg16(),\n",
    "    \"squeezenet\": models.squeezenet1_0(),\n",
    "    \"densenet161\": models.densenet161(),\n",
    "    \"inception_v3\": models.inception_v3(),\n",
    "    \"googlenet\": models.googlenet(),\n",
    "    \"shufflenet_v2\": models.shufflenet_v2_x1_0(),\n",
    "    \"mobilenet_v2\": models.mobilenet_v2(),\n",
    "    \"resnext50_32x4d\": models.resnext50_32x4d(),\n",
    "    \"wide_resnet50_2\": models.wide_resnet50_2(),\n",
    "    \"mnasnet\": models.mnasnet1_0()\n",
    "}\n",
    "\n",
    "# load predictor\n",
    "predictor = nn_meter.load_latency_predictor(predictor_name, predictor_version)\n",
    "\n",
    "for model_name in torchvision_models:\n",
    "    latency = predictor.predict(torchvision_models[model_name], model_type=\"torch\", input_shape=(1, 3, 224, 224)) \n",
    "    print(f'[RESULT] predict latency for {model_name}: {latency} ms')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[RESULT] predict latency for resnet18: 39.32351677226426 ms\n",
      "[RESULT] predict latency for alexnet: 13.126684104716283 ms\n",
      "[RESULT] predict latency for vgg16: 219.2647723703139 ms\n",
      "[RESULT] predict latency for squeezenet: 18.674223659837843 ms\n",
      "[RESULT] predict latency for densenet161: 186.56037984132988 ms\n",
      "[RESULT] predict latency for inception_v3: 127.98419924992326 ms\n",
      "[RESULT] predict latency for googlenet: 32.758087458683384 ms\n",
      "[RESULT] predict latency for shufflenet_v2: 5.423898780782251 ms\n",
      "[RESULT] predict latency for mobilenet_v2: 9.920667346583885 ms\n",
      "[RESULT] predict latency for resnext50_32x4d: 230.96098225315293 ms\n",
      "[RESULT] predict latency for wide_resnet50_2: 230.96098225315293 ms\n",
      "[RESULT] predict latency for mnasnet: 11.630591102084342 ms\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Use nn-Meter for ONNX File"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import nn_meter\n",
    "\n",
    "# download data and unzip\n",
    "ppath = os.path.join(__test_models_folder__, \"onnx_models\")\n",
    "if not os.path.isdir(ppath):\n",
    "    os.mkdir(ppath)\n",
    "    url = \"https://github.com/microsoft/nn-Meter/releases/download/v1.0-data/onnx_models.zip\"\n",
    "    nn_meter.download_from_url(url, ppath)\n",
    "\n",
    "test_model_list = glob(ppath + \"/**.onnx\")\n",
    "\n",
    "# load predictor\n",
    "predictor = nn_meter.load_latency_predictor(predictor_name, predictor_version)\n",
    "\n",
    "# predict latency\n",
    "result = {}\n",
    "for test_model in test_model_list:\n",
    "    latency = predictor.predict(test_model, model_type=\"onnx\") # in unit of ms\n",
    "    result[os.path.basename(test_model)] = latency\n",
    "    print(f'[RESULT] predict latency for {os.path.basename(test_model)}: {latency} ms')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[RESULT] predict latency for alexnet_0.onnx: 13.12668410471628 ms\n",
      "[RESULT] predict latency for densenet_0.onnx: 186.5603798413299 ms\n",
      "[RESULT] predict latency for googlenet_0.onnx: 32.758087458683384 ms\n",
      "[RESULT] predict latency for mnasnet_0.onnx: 11.63059110208434 ms\n",
      "[RESULT] predict latency for mobilenetv2_0.onnx: 9.920667346583883 ms\n",
      "[RESULT] predict latency for mobilenetv3large_0.onnx: 12.548914975618422 ms\n",
      "[RESULT] predict latency for mobilenetv3small_0.onnx: 6.705541180860482 ms\n",
      "[RESULT] predict latency for resnet18_0.onnx: 39.32351677226426 ms\n",
      "[RESULT] predict latency for shufflenetv2_0.onnx: 5.423898780782251 ms\n",
      "[RESULT] predict latency for squeezenet_0.onnx: 18.674223659837843 ms\n",
      "[RESULT] predict latency for vgg16_0.onnx: 219.26477237031392 ms\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Use nn-Meter for nn-Meter IR Graph"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import nn_meter\n",
    "\n",
    "# download data and unzip\n",
    "ppath = os.path.join(__test_models_folder__, \"nnmeter_ir_graphs\")\n",
    "if not os.path.isdir(ppath):\n",
    "    os.mkdir(ppath)\n",
    "    url = \"https://github.com/microsoft/nn-Meter/releases/download/v1.0-data/ir_graphs.zip\"\n",
    "    nn_meter.download_from_url(url, ppath)\n",
    "\n",
    "test_model_list = glob(ppath + \"/**.json\")\n",
    "\n",
    "# load predictor\n",
    "predictor = nn_meter.load_latency_predictor(predictor_name, predictor_version)\n",
    "\n",
    "# predict latency\n",
    "result = {}\n",
    "for test_model in test_model_list:\n",
    "    latency = predictor.predict(test_model, model_type=\"nnmeter-ir\") # in unit of ms\n",
    "    result[os.path.basename(test_model)] = latency\n",
    "    print(f'[RESULT] predict latency for {os.path.basename(test_model)}: {latency} ms')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[RESULT] predict latency for alexnet_0.json: 13.124763483485058 ms\n",
      "[RESULT] predict latency for densenet_0.json: 73.65728637938379 ms\n",
      "[RESULT] predict latency for googlenet_0.json: 34.508159026365064 ms\n",
      "[RESULT] predict latency for mnasnet_0.json: 13.72939336097471 ms\n",
      "[RESULT] predict latency for mobilenetv1_0.json: 13.972147254154745 ms\n",
      "[RESULT] predict latency for mobilenetv2_0.json: 10.15371207191722 ms\n",
      "[RESULT] predict latency for mobilenetv3large_0.json: 9.989918007478074 ms\n",
      "[RESULT] predict latency for mobilenetv3small_0.json: 4.489849402954042 ms\n",
      "[RESULT] predict latency for proxylessnas_0.json: 12.509469696629518 ms\n",
      "[RESULT] predict latency for resnet18_0.json: 39.32351677226428 ms\n",
      "[RESULT] predict latency for resnet34_0.json: 74.88913912781982 ms\n",
      "[RESULT] predict latency for resnet50_0.json: 91.73126828870865 ms\n",
      "[RESULT] predict latency for shufflenetv2_0.json: 5.423898780782249 ms\n",
      "[RESULT] predict latency for squeezenet_0.json: 18.074222853615616 ms\n",
      "[RESULT] predict latency for vgg11_0.json: 109.77864175998361 ms\n",
      "[RESULT] predict latency for vgg13_0.json: 158.909608204428 ms\n",
      "[RESULT] predict latency for vgg16_0.json: 219.24169918220582 ms\n",
      "[RESULT] predict latency for vgg19_0.json: 279.5737901599835 ms\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.10 64-bit ('py36': conda)"
  },
  "interpreter": {
   "hash": "725f784512f384579f6470be215ba42b52a9bbd25ecead6d24b2cff5bb6ad2c7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}